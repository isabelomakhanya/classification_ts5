{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-02T23:35:44.754705Z",
     "iopub.status.busy": "2021-06-02T23:35:44.754249Z",
     "iopub.status.idle": "2021-06-02T23:35:44.775038Z",
     "shell.execute_reply": "2021-06-02T23:35:44.773868Z",
     "shell.execute_reply.started": "2021-06-02T23:35:44.754668Z"
    },
    "papermill": {
     "duration": 0.029085,
     "end_time": "2021-06-10T11:00:29.318311",
     "exception": false,
     "start_time": "2021-06-10T11:00:29.289226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# EDSA - Climate Change Belief Analysis 2021\n",
    "### Team TS5 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T16:17:48.062355Z",
     "iopub.status.busy": "2021-06-03T16:17:48.06194Z",
     "iopub.status.idle": "2021-06-03T16:17:48.069466Z",
     "shell.execute_reply": "2021-06-03T16:17:48.067878Z",
     "shell.execute_reply.started": "2021-06-03T16:17:48.062321Z"
    },
    "papermill": {
     "duration": 0.027386,
     "end_time": "2021-06-10T11:00:29.373653",
     "exception": false,
     "start_time": "2021-06-10T11:00:29.346267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div align=\"center\" style=\"width: 1000px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/CPNMorgan/Team_TS5_JHB_Classification/main/climat change.jpg\"\n",
    "     alt=\"Titanic\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=1000px/>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027351,
     "end_time": "2021-06-10T11:00:29.428674",
     "exception": false,
     "start_time": "2021-06-10T11:00:29.401323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Challange Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027413,
     "end_time": "2021-06-10T11:00:29.483654",
     "exception": false,
     "start_time": "2021-06-10T11:00:29.456241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Many companies are built around lessening oneâ€™s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
    "\n",
    "With this context, EDSA is challenging you during the Classification Sprint with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02766,
     "end_time": "2021-06-10T11:00:29.538981",
     "exception": false,
     "start_time": "2021-06-10T11:00:29.511321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.027694,
     "end_time": "2021-06-10T11:00:29.594317",
     "exception": false,
     "start_time": "2021-06-10T11:00:29.566623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T16:23:06.285173Z",
     "iopub.status.busy": "2021-06-03T16:23:06.284666Z",
     "iopub.status.idle": "2021-06-03T16:23:06.289831Z",
     "shell.execute_reply": "2021-06-03T16:23:06.288702Z",
     "shell.execute_reply.started": "2021-06-03T16:23:06.285137Z"
    },
    "papermill": {
     "duration": 0.027728,
     "end_time": "2021-06-10T11:00:29.650136",
     "exception": false,
     "start_time": "2021-06-10T11:00:29.622408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:29.765450Z",
     "iopub.status.busy": "2021-06-10T11:00:29.764243Z",
     "iopub.status.idle": "2021-06-10T11:00:32.254609Z",
     "shell.execute_reply": "2021-06-10T11:00:32.253907Z",
     "shell.execute_reply.started": "2021-06-10T10:33:10.870622Z"
    },
    "papermill": {
     "duration": 2.521345,
     "end_time": "2021-06-10T11:00:32.254770",
     "exception": false,
     "start_time": "2021-06-10T11:00:29.733425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "#from plotly import graph_objs as go\n",
    "import seaborn as sns\n",
    "from plotly import graph_objs as go\n",
    "\n",
    "#nlp\n",
    "import string\n",
    "import re    #for regex\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "#import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer   \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_union\n",
    "from wordcloud import WordCloud \n",
    "from collections import Counter\n",
    "\n",
    "#models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:32.316907Z",
     "iopub.status.busy": "2021-06-10T11:00:32.316180Z",
     "iopub.status.idle": "2021-06-10T11:00:32.494847Z",
     "shell.execute_reply": "2021-06-10T11:00:32.494240Z",
     "shell.execute_reply.started": "2021-06-10T08:50:18.225298Z"
    },
    "papermill": {
     "duration": 0.211607,
     "end_time": "2021-06-10T11:00:32.495049",
     "exception": false,
     "start_time": "2021-06-10T11:00:32.283442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data imported succesfully\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "print('Data imported succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>message</th>\n      <th>tweetid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n      <td>625221</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>It's not like we lack evidence of anthropogeni...</td>\n      <td>126103</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>RT @RawStory: Researchers say we have three ye...</td>\n      <td>698562</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n      <td>573736</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n      <td>466954</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             message  tweetid\n",
       "0  Europe will now be looking to China to make su...   169760\n",
       "1  Combine this with the polling of staffers re c...    35326\n",
       "2  The scary, unimpeachable evidence that climate...   224985\n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>message</th>\n      <th>tweetid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Europe will now be looking to China to make su...</td>\n      <td>169760</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Combine this with the polling of staffers re c...</td>\n      <td>35326</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The scary, unimpeachable evidence that climate...</td>\n      <td>224985</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n      <td>476263</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n      <td>872928</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:32.697062Z",
     "iopub.status.busy": "2021-06-10T11:00:32.696382Z",
     "iopub.status.idle": "2021-06-10T11:00:32.701403Z",
     "shell.execute_reply": "2021-06-10T11:00:32.701855Z",
     "shell.execute_reply.started": "2021-06-10T06:17:07.701621Z"
    },
    "papermill": {
     "duration": 0.036715,
     "end_time": "2021-06-10T11:00:32.702052",
     "exception": false,
     "start_time": "2021-06-10T11:00:32.665337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((15819, 3), (10546, 2))"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#check the shape of the data\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:32.763657Z",
     "iopub.status.busy": "2021-06-10T11:00:32.762978Z",
     "iopub.status.idle": "2021-06-10T11:00:32.772620Z",
     "shell.execute_reply": "2021-06-10T11:00:32.772132Z",
     "shell.execute_reply.started": "2021-06-10T06:17:20.021929Z"
    },
    "papermill": {
     "duration": 0.041649,
     "end_time": "2021-06-10T11:00:32.772773",
     "exception": false,
     "start_time": "2021-06-10T11:00:32.731124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 1,  2,  0, -1])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "#unique sentiments\n",
    "train_df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       " 1    8530\n",
       " 2    3640\n",
       " 0    2353\n",
       "-1    1296\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "count    15819.000000\n",
       "mean         0.917504\n",
       "std          0.836537\n",
       "min         -1.000000\n",
       "25%          1.000000\n",
       "50%          1.000000\n",
       "75%          1.000000\n",
       "max          2.000000\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_df['sentiment'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:32.844017Z",
     "iopub.status.busy": "2021-06-10T11:00:32.838561Z",
     "iopub.status.idle": "2021-06-10T11:00:32.847407Z",
     "shell.execute_reply": "2021-06-10T11:00:32.846815Z",
     "shell.execute_reply.started": "2021-06-10T06:17:39.289686Z"
    },
    "papermill": {
     "duration": 0.045151,
     "end_time": "2021-06-10T11:00:32.847546",
     "exception": false,
     "start_time": "2021-06-10T11:00:32.802395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(sentiment    0\n",
       " message      0\n",
       " tweetid      0\n",
       " dtype: int64,\n",
       " message    0\n",
       " tweetid    0\n",
       " dtype: int64)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "#check for nulls\n",
    "train_df.isnull().sum(), test_df.isnull().sum()"
   ]
  },
  {
   "source": [
    "# CLEANING"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating copy of train_df\n",
    "df = train_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221   \n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103   \n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562   \n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736   \n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954   \n",
       "\n",
       "                                       clean_message  \n",
       "0  polyscimajor epa chief think carbon dioxide ma...  \n",
       "1    like lack evidence anthropogenic global warming  \n",
       "2  rawstory researchers say three years act clima...  \n",
       "3  todayinmaker wired pivotal year war climate ch...  \n",
       "4  soynoviodetodas racist sexist climate change d...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>message</th>\n      <th>tweetid</th>\n      <th>clean_message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n      <td>625221</td>\n      <td>polyscimajor epa chief think carbon dioxide ma...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>It's not like we lack evidence of anthropogeni...</td>\n      <td>126103</td>\n      <td>like lack evidence anthropogenic global warming</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>RT @RawStory: Researchers say we have three ye...</td>\n      <td>698562</td>\n      <td>rawstory researchers say three years act clima...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n      <td>573736</td>\n      <td>todayinmaker wired pivotal year war climate ch...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n      <td>466954</td>\n      <td>soynoviodetodas racist sexist climate change d...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# 1. Lowercase text\n",
    "# 2. Remove whitespace\n",
    "# 3. Remove numbers\n",
    "# 4. Remove special characters\n",
    "# 5. Remove emails\n",
    "# 6. Remove stop words\n",
    "# 7. Remove NAN\n",
    "# 8. Remove weblinks\n",
    "# 9. Expand contractions (if possible not necessary)\n",
    "# 10. Tokenize\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer, LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    # stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    # lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "df['clean_message']=df['message'].map(lambda s:preprocess(s)) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       sentiment                                            message  tweetid  \\\n",
       "15814          1  RT @ezlusztig: They took down the material on ...    22001   \n",
       "15815          2  RT @washingtonpost: How climate change could b...    17856   \n",
       "15816          0  notiven: RT: nytimesworld :What does Trump act...   384248   \n",
       "15817         -1  RT @sara8smiles: Hey liberals the climate chan...   819732   \n",
       "15818          0  RT @Chet_Cannon: .@kurteichenwald's 'climate c...   806319   \n",
       "\n",
       "                                           clean_message  \n",
       "15814  ezlusztig took material global warming lgbt ri...  \n",
       "15815  washingtonpost climate change could breaking m...  \n",
       "15816  notiven nytimesworld trump actually believe cl...  \n",
       "15817  sarasmiles hey liberals climate change crap ho...  \n",
       "15818  chet_cannon kurteichenwald climate change equa...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>message</th>\n      <th>tweetid</th>\n      <th>clean_message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15814</th>\n      <td>1</td>\n      <td>RT @ezlusztig: They took down the material on ...</td>\n      <td>22001</td>\n      <td>ezlusztig took material global warming lgbt ri...</td>\n    </tr>\n    <tr>\n      <th>15815</th>\n      <td>2</td>\n      <td>RT @washingtonpost: How climate change could b...</td>\n      <td>17856</td>\n      <td>washingtonpost climate change could breaking m...</td>\n    </tr>\n    <tr>\n      <th>15816</th>\n      <td>0</td>\n      <td>notiven: RT: nytimesworld :What does Trump act...</td>\n      <td>384248</td>\n      <td>notiven nytimesworld trump actually believe cl...</td>\n    </tr>\n    <tr>\n      <th>15817</th>\n      <td>-1</td>\n      <td>RT @sara8smiles: Hey liberals the climate chan...</td>\n      <td>819732</td>\n      <td>sarasmiles hey liberals climate change crap ho...</td>\n    </tr>\n    <tr>\n      <th>15818</th>\n      <td>0</td>\n      <td>RT @Chet_Cannon: .@kurteichenwald's 'climate c...</td>\n      <td>806319</td>\n      <td>chet_cannon kurteichenwald climate change equa...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029839,
     "end_time": "2021-06-10T11:00:32.992482",
     "exception": false,
     "start_time": "2021-06-10T11:00:32.962643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030734,
     "end_time": "2021-06-10T11:00:33.053423",
     "exception": false,
     "start_time": "2021-06-10T11:00:33.022689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " Exploratory Data Analysis (EDA) refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:33.127588Z",
     "iopub.status.busy": "2021-06-10T11:00:33.126901Z",
     "iopub.status.idle": "2021-06-10T11:00:33.129384Z",
     "shell.execute_reply": "2021-06-10T11:00:33.129828Z",
     "shell.execute_reply.started": "2021-06-10T08:50:51.730910Z"
    },
    "papermill": {
     "duration": 0.044893,
     "end_time": "2021-06-10T11:00:33.130022",
     "exception": false,
     "start_time": "2021-06-10T11:00:33.085129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#copy_df = train_df.copy()\n",
    "# Labeling the target\n",
    "#train_df['sentiment'] = [['Negative(-1)', 'Neutral(0)', 'Positive(1)', 'News(2)'][x+1] for x in train_df['sentiment']]\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discreption of Sentiments:\n",
    "- 2 News: the tweet links to factual news about climate change\n",
    "- 1 Positive: the tweet supports the belief of man-made climate change\n",
    "- 0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "- -1 Negative: the tweet does not believe in man-made climate change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:33.194135Z",
     "iopub.status.busy": "2021-06-10T11:00:33.193514Z",
     "iopub.status.idle": "2021-06-10T11:00:33.245882Z",
     "shell.execute_reply": "2021-06-10T11:00:33.245334Z",
     "shell.execute_reply.started": "2021-06-10T08:51:10.368730Z"
    },
    "papermill": {
     "duration": 0.085286,
     "end_time": "2021-06-10T11:00:33.246038",
     "exception": false,
     "start_time": "2021-06-10T11:00:33.160752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist = train_df.groupby('sentiment').count()['clean_message'].reset_index().sort_values(by='clean_message',ascending=True)\n",
    "dist.style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count of words per sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x='sentiment',data=train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- The graph a bove shows that the words in the positive sentiment are higher compare to others, with 8000 words, were by the rest are below 4000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031158,
     "end_time": "2021-06-10T11:00:33.573439",
     "exception": false,
     "start_time": "2021-06-10T11:00:33.542281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Funnel chart to get more insight from sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:33.641584Z",
     "iopub.status.busy": "2021-06-10T11:00:33.640896Z",
     "iopub.status.idle": "2021-06-10T11:00:33.750759Z",
     "shell.execute_reply": "2021-06-10T11:00:33.750107Z",
     "shell.execute_reply.started": "2021-06-10T08:51:29.450713Z"
    },
    "papermill": {
     "duration": 0.146097,
     "end_time": "2021-06-10T11:00:33.750900",
     "exception": false,
     "start_time": "2021-06-10T11:00:33.604803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(go.Funnelarea(\n",
    "    text =train_df.sentiment,\n",
    "    values = dist.clean_message,\n",
    "    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n",
    "    ))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "- Negative(-1) Sentiment shows the less of counted words with the percentage of 8.19\n",
    "- Neutral(0) shows 14.9 percents of counted words\n",
    "- News(2) shows 23 percents of counted words\n",
    "- And Positive comments shows the highest percentage of 53.9 of counted words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033422,
     "end_time": "2021-06-10T11:00:33.817700",
     "exception": false,
     "start_time": "2021-06-10T11:00:33.784278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## checking average length of each sentiment catergory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:33.907815Z",
     "iopub.status.busy": "2021-06-10T11:00:33.907160Z",
     "iopub.status.idle": "2021-06-10T11:00:34.330572Z",
     "shell.execute_reply": "2021-06-10T11:00:34.331114Z",
     "shell.execute_reply.started": "2021-06-10T09:31:16.088029Z"
    },
    "papermill": {
     "duration": 0.480759,
     "end_time": "2021-06-10T11:00:34.331303",
     "exception": false,
     "start_time": "2021-06-10T11:00:33.850544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x='sentiment', y=train_df['clean_message'].apply(len) ,data = train_df, palette='viridis')\n",
    "plt.ylabel('avg_Length')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.title('Average Length of Cleaned_Message by Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    " -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.033457,
     "end_time": "2021-06-10T11:00:34.408250",
     "exception": false,
     "start_time": "2021-06-10T11:00:34.374793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweetid = train_df['tweetid']\n",
    "#df_test = df_test.drop('Index', axis = 1)\n",
    "\n",
    "len(tweetid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common words in our Target-Selected Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['new_lis'] = train_df['clean_message'].apply(lambda x:str(x).split())\n",
    "words = Counter([item for sublist in train_df['new_lis'] for item in sublist])\n",
    "new = pd.DataFrame(words.most_common(10))\n",
    "new.columns = ['Common_words','count']\n",
    "new.style.background_gradient(cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.bar(new, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', \n",
    "             width=600, height=600,color='Common_words')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    " -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common words Sentiments Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Positive = train_df[train_df['sentiment']==1]\n",
    "Negative = train_df[train_df['sentiment']==-1]\n",
    "Neutral = train_df[train_df['sentiment']==0]\n",
    "News = train_df[train_df['sentiment']==2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common positive(1) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MosT common positive words\n",
    "top = Counter([item for sublist in Positive['new_lis'] for item in sublist])\n",
    "positive_w = pd.DataFrame(top.most_common(10))\n",
    "positive_w.columns = ['Common_words','count']\n",
    "positive_w.style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(positive_w, x=\"count\", y=\"Common_words\", title='Most Commmon Positive(1) Words', orientation='h', \n",
    "             width=600, height=600,color='Common_words')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    " -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top = Counter([item for sublist in Negative['new_lis'] for item in sublist])\n",
    "negative = pd.DataFrame(top.most_common(10))\n",
    "negative = negative.iloc[1:,:]\n",
    "negative.columns = ['Common_words','count']\n",
    "negative.style.background_gradient(cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(negative, x=\"count\", y=\"Common_words\", title='Most Commmon Negative(-1) Words', orientation='h', \n",
    "             width=600, height=600,color='Common_words')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    " -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common neutral(0) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = Counter([item for sublist in Neutral['new_lis'] for item in sublist])\n",
    "neutral = pd.DataFrame(top.most_common(10))\n",
    "neutral = neutral.iloc[1:,:]\n",
    "neutral.columns = ['Common_words','count']\n",
    "neutral.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(neutral, x=\"count\", y=\"Common_words\", title='Most Commmon Neutral(0) Words', orientation='h', \n",
    "             width=600, height=600,color='Common_words')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    " -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common news(2) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = Counter([item for sublist in News['new_lis'] for item in sublist])\n",
    "news = pd.DataFrame(top.most_common(10))\n",
    "news = news.iloc[1:,:]\n",
    "news.columns = ['Common_words','count']\n",
    "news.style.background_gradient(cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(news, x=\"count\", y=\"Common_words\", title='Most Commmon Neutral(0) Words', orientation='h', \n",
    "             width=600, height=600,color='Common_words')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    " -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting number of Emojis used in the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji\n",
    "import emoji\n",
    "emojis_df=[]\n",
    "for string in test_df['clean_message']:\n",
    "    my_str = str(string)\n",
    "    for each in my_str:\n",
    "        if each in emoji.UNICODE_EMOJI['en'].keys():\n",
    "            emojis_df.append(each)\n",
    "freq = (Counter(i for sub in emojis_df for i in set(sub))) \n",
    "sort_orders = sorted(freq.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sort_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "- The most common emoji is the 'face with tears of joy' is an emoji featuring a jovial face laughing, while also crying out tears\n",
    "- We have other emjis that illustrate the impacts of global warmaing on weather and climate change, e.g 'earth,fire,flowers and snowflake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud of most common positive words used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=250,background_color='white', width=1500, height=700).generate(' '.join(Positive[Positive['sentiment'] == 1]\n",
    "                                          ['clean_message']))\n",
    "plt.figure( figsize=(16,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('Positive Tweets')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    " -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud of most common negative words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=250,background_color='white', width=1500, height=700).generate(' '.join(Negative[Negative['sentiment'] == -1]\n",
    "                                          ['clean_message']))\n",
    "plt.figure( figsize=(16,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('Negative Tweets')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    " -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud of most common neutral words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=250,background_color='white', width=1500, height=700).generate(' '.join(Neutral[Neutral['sentiment'] == 0]\n",
    "                                          ['clean_message']))\n",
    "plt.figure( figsize=(16,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('Neutral Tweets')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    " -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud of most common neutral words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=250,background_color='white', width=1500, height=700).generate(' '.join(News[News['sentiment'] == 2]\n",
    "                                          ['clean_message']))\n",
    "plt.figure( figsize=(16,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('News Tweets')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud of most common words used 'Overall'\n",
    "- Wordcloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_msg = \" \".join(tweet for tweet in train_df.clean_message)\n",
    "train_wordcloud = WordCloud(max_font_size=250,\n",
    "                            background_color=\"black\",\n",
    "                            width=1500,\n",
    "                            height=700,\n",
    "                            collocations=False,\n",
    "                            colormap='Paired').generate(train_msg)\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.imshow(train_wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    " - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate minority and majority classes\n",
    "majority_sentiment = train_df[train_df['sentiment']== 1]\n",
    "minority_sentiment_0 = train_df[train_df['sentiment']== 0]\n",
    "minority_sentiment_2 = train_df[train_df['sentiment']== 2]\n",
    "minority_sentiment_neg1 = train_df[train_df['sentiment'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Upsample minority\n",
    "minority_sentiment_0_minority = resample(minority_sentiment_0,\n",
    "                          replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                          n_samples=len(majority_sentiment), # match number in minority class\n",
    "                          random_state=42) # reproducible results\n",
    "\n",
    "minority_sentiment_2_minority = resample(minority_sentiment_2,\n",
    "                                         replace = True,\n",
    "                                         n_samples=len(majority_sentiment),\n",
    "                                         random_state=42)\n",
    "\n",
    "minority_sentiment_neg1_minority = resample(minority_sentiment_neg1,\n",
    "                                   replace=True,\n",
    "                                   n_samples=len(majority_sentiment),\n",
    "                                   random_state=42)\n",
    "\n",
    "# Combine upsampled minority class with majority class\n",
    "train_df = pd.concat([majority_sentiment, minority_sentiment_0_minority,minority_sentiment_2_minority,minority_sentiment_neg1_minority])\n",
    "\n",
    "# Check new class counts\n",
    "train_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the table of balanced data in clean_text from each language Id\n",
    "dist = train_df.groupby('sentiment').count()['clean_message'].reset_index().sort_values(by='clean_message',ascending=True)\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprosessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train (Validation) Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df['clean_message'].values\n",
    "y = train_df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models \n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "clf_dict = [LinearSVC(random_state=42), KNeighborsClassifier(n_neighbors=5),DecisionTreeClassifier(max_depth=5),\n",
    "                        RandomForestClassifier(max_depth=5, n_estimators=10,\n",
    "                        max_features=1), ComplementNB(),AdaBoostClassifier()\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling(clf_dict, X_train, y_train, X_val, y_val):\n",
    "\n",
    "    models = {}\n",
    "    for clf in clf_dict:\n",
    "        clf_text = Pipeline([('tfidf', TfidfVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))),('clf', clf)])\n",
    "        \n",
    "        clf_text.fit(X_train, y_train)\n",
    "        predictions = clf_text.predict(X_val)\n",
    "\n",
    "        # Output for each model\n",
    "        models[clf.__class__.__name__] = {\n",
    "            'F1-Macro': metrics.f1_score(y_val,predictions,average='macro'),\n",
    "            'F1-Mccuracy': metrics.f1_score(y_val, predictions,average='micro'),\n",
    "            'F1-Weighted': metrics.f1_score(y_val,predictions,average='weighted')}\n",
    "\n",
    "    return pd.DataFrame.from_dict(models, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying all the models\n",
    "classi = modeling(clf_dict, X_train, y_train, X_val, y_val)\n",
    "display_df = classi.sort_values('F1-Macro', ascending=False)\n",
    "display_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline for the gridsearch\n",
    "param_grid = {'alpha': [0.1, 1, 5, 11]}  # parameter grid\n",
    "\n",
    "pipe_l = Pipeline([('tfidf', TfidfVectorizer(min_df=2, max_df=0.9, ngram_range=(1, 3))), ('mnb', GridSearchCV(MultinomialNB(),  param_grid=param_grid, cv=5, n_jobs=-1, scoring='f1_weighted'))])\n",
    "\n",
    "# Fitting the model\n",
    "\n",
    "pipe_l.fit(X_train, y_train) \n",
    "\n",
    "# predicting the fit\n",
    "y_pred_pipe = pipe_l.predict(X_val)  \n",
    "\n",
    "print(classification_report(y_val, y_pred_pipe))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['message']\n",
    "test_df['sentiment'] = pipe_l.predict(test_df['message'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df.drop('message', axis = 1)\n",
    "\n",
    "test_df[['tweetid','sentiment']].to_csv('submission8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission = pd.read_csv('submission8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037324,
     "end_time": "2021-06-10T11:02:46.044445",
     "exception": false,
     "start_time": "2021-06-10T11:02:46.007121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## References\n",
    "- Websites used for insipiration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037772,
     "end_time": "2021-06-10T11:02:46.119164",
     "exception": false,
     "start_time": "2021-06-10T11:02:46.081392",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- analyticsvidhya: https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/\n",
    "- towardsdatascience: https://towardsdatascience.com/how-to-efficiently-remove-punctuations-from-a-string-899ad4a059fb\n",
    "- codegrepper:  https://www.codegrepper.com/code-examples/python/pandas+series+remove+punctuation\n",
    "- https://stackoverflow.com/questions/54396405/how-can-i-preprocess-nlp-text-lowercase-remove-special-characters-remove-numb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.0364,
     "end_time": "2021-06-10T11:02:46.193094",
     "exception": false,
     "start_time": "2021-06-10T11:02:46.156694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2cb31fa55ae00465923d9b187a5051f66c494d866be42a4c03d35b8bacfe6992"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('.env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 145.811705,
   "end_time": "2021-06-10T11:02:47.040917",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-10T11:00:21.229212",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}