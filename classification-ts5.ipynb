{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# EDSA - Climate Change Belief Analysis 2021\n",
    "### Team TS5 Classification"
   ],
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-02T23:35:44.754705Z",
     "iopub.status.busy": "2021-06-02T23:35:44.754249Z",
     "iopub.status.idle": "2021-06-02T23:35:44.775038Z",
     "shell.execute_reply": "2021-06-02T23:35:44.773868Z",
     "shell.execute_reply.started": "2021-06-02T23:35:44.754668Z"
    },
    "papermill": {
     "duration": 0.029085,
     "end_time": "2021-06-10T11:00:29.318311",
     "exception": false,
     "start_time": "2021-06-10T11:00:29.289226",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div align=\"center\" style=\"width: 1000px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/CPNMorgan/Team_TS5_JHB_Classification/main/climat change.jpg\"\n",
    "     alt=\"Titanic\"\n",
    "     style=\"float: center; padding-bottom=0.5em\"\n",
    "     width=1000px/>\n",
    "\n",
    "</div>"
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T16:17:48.062355Z",
     "iopub.status.busy": "2021-06-03T16:17:48.06194Z",
     "iopub.status.idle": "2021-06-03T16:17:48.069466Z",
     "shell.execute_reply": "2021-06-03T16:17:48.067878Z",
     "shell.execute_reply.started": "2021-06-03T16:17:48.062321Z"
    },
    "papermill": {
     "duration": 0.027386,
     "end_time": "2021-06-10T11:00:29.373653",
     "exception": false,
     "start_time": "2021-06-10T11:00:29.346267",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Challenge Description"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.027351,
     "end_time": "2021-06-10T11:00:29.428674",
     "exception": false,
     "start_time": "2021-06-10T11:00:29.401323",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Many companies are built around lessening oneâ€™s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
    "\n",
    "With this context, EDSA is challenging you during the Classification Sprint with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies."
   ],
   "metadata": {
    "papermill": {
     "duration": 0.027413,
     "end_time": "2021-06-10T11:00:29.483654",
     "exception": false,
     "start_time": "2021-06-10T11:00:29.456241",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.02766,
     "end_time": "2021-06-10T11:00:29.538981",
     "exception": false,
     "start_time": "2021-06-10T11:00:29.511321",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Theoretical (LogisticRegression, )\n",
    "Problem/Hypothesis\n",
    "\n"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.027694,
     "end_time": "2021-06-10T11:00:29.594317",
     "exception": false,
     "start_time": "2021-06-10T11:00:29.566623",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output value of a logistic regression model refers to the probability that the observation in question belongs to class 1. The output values all fall between 0 and 1, which is all very well. But at what threshold value do we decide that a probability is too low to be assigned to class 1? Usually, we pick 0.5. That is:\n",
    "\n",
    "- Values greater than or equal to 0.5 are assigned to class 1; and\n",
    "- Values less than 0.5 are assigned to class 0.\n",
    "\n",
    "This output needs to hold for all values of X. In other words, regardless of the value of X, we need the output to be a value between 0 and 1. The function that takes care of all this is defined as follows:\n",
    "\n",
    "$$P(X) = \\displaystyle \\frac{e^{\\beta_0 + \\beta_1 X}}{1+e^{\\beta_0 + \\beta_1 X}}$$\n",
    "\n",
    "where $P(X)$ is the probability of X belonging to class 1, and $\\beta_0$ and $\\beta_1$ are the intercept and regression coefficient respectively, just like in a linear regression model. After a bit of manipulation we arrive at:\n",
    "\n",
    "\\begin{align}\n",
    "1 - P(X) &= \\displaystyle \\frac{1}{1+e^{\\beta_0 + \\beta_1 X}} \\\\\n",
    "\\therefore \\log \\left( \\frac{P(X)}{1-P(X)} \\right) &= {\\beta_0 + \\beta_1 X}\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"useful_info/image5-13.jpg\" align=\"center\"/>\n",
    "\n",
    "The term on the left is known as the **log odds ratio**. Without the log sign in front of it, it is known simply as the odds ratio. While $P(X)$ is bounded between 0 and 1, the odds ratio is bounded between 0 and $\\infty$. \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing Packages"
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T16:23:06.285173Z",
     "iopub.status.busy": "2021-06-03T16:23:06.284666Z",
     "iopub.status.idle": "2021-06-03T16:23:06.289831Z",
     "shell.execute_reply": "2021-06-03T16:23:06.288702Z",
     "shell.execute_reply.started": "2021-06-03T16:23:06.285137Z"
    },
    "papermill": {
     "duration": 0.027728,
     "end_time": "2021-06-10T11:00:29.650136",
     "exception": false,
     "start_time": "2021-06-10T11:00:29.622408",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#comet.ml\n",
    "import comet_ml\n",
    "from comet_ml import Experiment\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "#nlp\n",
    "import string\n",
    "import re    #for regex\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer, LancasterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer   \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_union\n",
    "from wordcloud import WordCloud \n",
    "from collections import Counter\n",
    "import emoji\n",
    "\n",
    "#models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:29.765450Z",
     "iopub.status.busy": "2021-06-10T11:00:29.764243Z",
     "iopub.status.idle": "2021-06-10T11:00:32.254609Z",
     "shell.execute_reply": "2021-06-10T11:00:32.253907Z",
     "shell.execute_reply.started": "2021-06-10T10:33:10.870622Z"
    },
    "papermill": {
     "duration": 2.521345,
     "end_time": "2021-06-10T11:00:32.254770",
     "exception": false,
     "start_time": "2021-06-10T11:00:29.733425",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialising Comet.ml Experiment Tracking"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "comet_ml.init()\n",
    "experiment = Experiment(\n",
    "            project_name=\"edsa-climate-change-sentiment-analysis/Default view\",\n",
    "            workspace=\"classification-ts5\",\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing Datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "print('Data imported succesfully')"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:32.316907Z",
     "iopub.status.busy": "2021-06-10T11:00:32.316180Z",
     "iopub.status.idle": "2021-06-10T11:00:32.494847Z",
     "shell.execute_reply": "2021-06-10T11:00:32.494240Z",
     "shell.execute_reply.started": "2021-06-10T08:50:18.225298Z"
    },
    "papermill": {
     "duration": 0.211607,
     "end_time": "2021-06-10T11:00:32.495049",
     "exception": false,
     "start_time": "2021-06-10T11:00:32.283442",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# First Look at the Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#check the shape of the data\n",
    "train_df.shape, test_df.shape"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:32.697062Z",
     "iopub.status.busy": "2021-06-10T11:00:32.696382Z",
     "iopub.status.idle": "2021-06-10T11:00:32.701403Z",
     "shell.execute_reply": "2021-06-10T11:00:32.701855Z",
     "shell.execute_reply.started": "2021-06-10T06:17:07.701621Z"
    },
    "papermill": {
     "duration": 0.036715,
     "end_time": "2021-06-10T11:00:32.702052",
     "exception": false,
     "start_time": "2021-06-10T11:00:32.665337",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#unique sentiments\n",
    "train_df['sentiment'].unique()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:32.763657Z",
     "iopub.status.busy": "2021-06-10T11:00:32.762978Z",
     "iopub.status.idle": "2021-06-10T11:00:32.772620Z",
     "shell.execute_reply": "2021-06-10T11:00:32.772132Z",
     "shell.execute_reply.started": "2021-06-10T06:17:20.021929Z"
    },
    "papermill": {
     "duration": 0.041649,
     "end_time": "2021-06-10T11:00:32.772773",
     "exception": false,
     "start_time": "2021-06-10T11:00:32.731124",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#samples per sentiment\n",
    "train_df['sentiment'].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#trainig data statistics\n",
    "train_df['sentiment'].describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#checking for nulls\n",
    "train_df.isnull().sum(), test_df.isnull().sum()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:32.844017Z",
     "iopub.status.busy": "2021-06-10T11:00:32.838561Z",
     "iopub.status.idle": "2021-06-10T11:00:32.847407Z",
     "shell.execute_reply": "2021-06-10T11:00:32.846815Z",
     "shell.execute_reply.started": "2021-06-10T06:17:39.289686Z"
    },
    "papermill": {
     "duration": 0.045151,
     "end_time": "2021-06-10T11:00:32.847546",
     "exception": false,
     "start_time": "2021-06-10T11:00:32.802395",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DATA CLEANING"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# creating copy of train_df\n",
    "df = train_df.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removing Noise"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_fil(sentence):\n",
    "    '''function removes noise/cleans text data'''\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\")\n",
    "    cleanr = re.compile('<.*?>') \n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocess_stemm(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(stem_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocess_lemm(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in filtered_words ]\n",
    "    return \" \".join(lemma_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cleaning the text messages and creates a new column named 'clean_message'\n",
    "df['clean_message']=df['message'].map(lambda s:preprocess_fil(s))\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# lemmatizes the cleaned text data and creates new column named 'Lemma\"\n",
    "df['Lemma']=df['message'].map(lambda s:preprocess_lemm(s)) \n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# stemmatize the cleaned text data and creates a new column named 'Stemm'\n",
    "df['stemm']=df['message'].map(lambda s:preprocess_stemm(s)) \n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# proves that the lemmatization is applied to the 'clean_message' column\n",
    "df['Lemma']==df['clean_message']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# proves that stemming is not equal to lemmatization\n",
    "df['Lemma']==df['stemm']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.029839,
     "end_time": "2021-06-10T11:00:32.992482",
     "exception": false,
     "start_time": "2021-06-10T11:00:32.962643",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Exploratory Data Analysis (EDA) refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.030734,
     "end_time": "2021-06-10T11:00:33.053423",
     "exception": false,
     "start_time": "2021-06-10T11:00:33.022689",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Labeling the target\n",
    "df['class_label'] = [['Negative(-1)', 'Neutral(0)', 'Positive(1)', 'News(2)'][x+1] for x in df['sentiment']]\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:33.127588Z",
     "iopub.status.busy": "2021-06-10T11:00:33.126901Z",
     "iopub.status.idle": "2021-06-10T11:00:33.129384Z",
     "shell.execute_reply": "2021-06-10T11:00:33.129828Z",
     "shell.execute_reply.started": "2021-06-10T08:50:51.730910Z"
    },
    "papermill": {
     "duration": 0.044893,
     "end_time": "2021-06-10T11:00:33.130022",
     "exception": false,
     "start_time": "2021-06-10T11:00:33.085129",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Discreption of Sentiments:\n",
    "- 2 News: the tweet links to factual news about climate change\n",
    "- 1 Positive: the tweet supports the belief of man-made climate change\n",
    "- 0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "- -1 Negative: the tweet does not believe in man-made climate change"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Count of words per sentiment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dist = df.groupby('class_label').count()['clean_message'].reset_index().sort_values(by='clean_message',ascending=False)\n",
    "dist.style.background_gradient(cmap='Blues')"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:33.194135Z",
     "iopub.status.busy": "2021-06-10T11:00:33.193514Z",
     "iopub.status.idle": "2021-06-10T11:00:33.245882Z",
     "shell.execute_reply": "2021-06-10T11:00:33.245334Z",
     "shell.execute_reply.started": "2021-06-10T08:51:10.368730Z"
    },
    "papermill": {
     "duration": 0.085286,
     "end_time": "2021-06-10T11:00:33.246038",
     "exception": false,
     "start_time": "2021-06-10T11:00:33.160752",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "labels = []\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x='class_label',data=df, palette=\"Blues_d\")\n",
    "plt.title('Count of Sentiments')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Relation between length of text and sentiment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# average length of words overall\n",
    "df['clean_message'].str.split().\\\n",
    "    apply(lambda x : [len(i) for i in x]).\\\n",
    "    map(lambda x : np.mean(x)).hist()\n",
    "plt.title('Avg number of words used per tweet')\n",
    "plt.xlabel('Number of words per tweet')\n",
    "plt.ylabel('Count of Tweets')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observations\n",
    "- The graph above shows that the words in the positive sentiment are higher compare to others, with 8000 words, were by the rest are below 4000"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Distribution of length of words per sentiment\n",
    "\n",
    "df['length_tweet'] = df['clean_message'].apply(len)\n",
    "h = sns.FacetGrid(df,col = 'class_label')\n",
    "h.map(plt.hist,'length_tweet')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Box plot visual of distribution between length of tweet vs class label\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=df['class_label'],\n",
    "            y=df.clean_message.str.split().apply(len),\n",
    "            data=df,\n",
    "            palette=\"Blues\")\n",
    "\n",
    "plt.title('No of Words per Tweet by Sentiment Class')\n",
    "plt.xlabel('Sentiment Class')\n",
    "plt.ylabel('Word Count per Tweet');\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Funnel chart to get more insight from sentiments"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.031158,
     "end_time": "2021-06-10T11:00:33.573439",
     "exception": false,
     "start_time": "2021-06-10T11:00:33.542281",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = go.Figure(go.Funnelarea(\n",
    "    text =dist.class_label,\n",
    "    values = dist.clean_message,\n",
    "    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n",
    "    ))\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:33.641584Z",
     "iopub.status.busy": "2021-06-10T11:00:33.640896Z",
     "iopub.status.idle": "2021-06-10T11:00:33.750759Z",
     "shell.execute_reply": "2021-06-10T11:00:33.750107Z",
     "shell.execute_reply.started": "2021-06-10T08:51:29.450713Z"
    },
    "papermill": {
     "duration": 0.146097,
     "end_time": "2021-06-10T11:00:33.750900",
     "exception": false,
     "start_time": "2021-06-10T11:00:33.604803",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation\n",
    "- Negative(-1) Sentiment shows the less of counted words with the percentage of 8.19\n",
    "- Neutral(0) shows 14.9 percents of counted words\n",
    "- News(2) shows 23 percents of counted words\n",
    "- And Positive comments shows the highest percentage of 53.9 of counted words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## checking average length of each sentiment catergory"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.033422,
     "end_time": "2021-06-10T11:00:33.817700",
     "exception": false,
     "start_time": "2021-06-10T11:00:33.784278",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# avarage length of words per sentiment category\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x='class_label', y=df['clean_message'].apply(len) ,data = df, palette='Blues_d')\n",
    "plt.ylabel('avg_Length')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.title('Average Length of Cleaned_Message by Sentiment')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:00:33.907815Z",
     "iopub.status.busy": "2021-06-10T11:00:33.907160Z",
     "iopub.status.idle": "2021-06-10T11:00:34.330572Z",
     "shell.execute_reply": "2021-06-10T11:00:34.331114Z",
     "shell.execute_reply.started": "2021-06-10T09:31:16.088029Z"
    },
    "papermill": {
     "duration": 0.480759,
     "end_time": "2021-06-10T11:00:34.331303",
     "exception": false,
     "start_time": "2021-06-10T11:00:33.850544",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#View of average length of tweet\n",
    "\n",
    "rel = df.groupby('sentiment').mean()\n",
    "round(rel,2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observations:\n",
    " - On avarage tweet length is between 70 - 80 words for each sentiment category"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Most Common words in our Target-Selected Message"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "df['new_lis'] = df['clean_message'].apply(lambda x:str(x).split())\n",
    "words = Counter([item for sublist in df['new_lis'] for item in sublist])\n",
    "new = pd.DataFrame(words.most_common(20))\n",
    "new.columns = ['Common_words','count']\n",
    "new.style.background_gradient(cmap='Blues')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import plotly.express as px\n",
    "fig = px.bar(new, x=\"count\", y=\"Common_words\", color_discrete_sequence =['blue']*len(df), title='Commmon Words in tweet messages', orientation='h', \n",
    "             width=600, height=600)\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observations:\n",
    " -"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Most common words Sentiments Wise"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Positive = df[df['sentiment']==1]\n",
    "Negative = df[df['sentiment']==-1]\n",
    "Neutral = df[train_df['sentiment']==0]\n",
    "News = df[df['sentiment']==2]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Most common positive(1) words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#MosT common positive words\n",
    "top = Counter([item for sublist in Positive['new_lis'] for item in sublist])\n",
    "positive_w = pd.DataFrame(top.most_common(20))\n",
    "positive_w.columns = ['Common_words','count']\n",
    "positive_w.style.background_gradient(cmap='Greens')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = px.bar(positive_w, x=\"count\", y=\"Common_words\", title='Most Commmon Positive(1) Words', orientation='h', \n",
    "             width=600, height=600)\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observations:\n",
    " -"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Most common negative words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "top = Counter([item for sublist in Negative['new_lis'] for item in sublist])\n",
    "negative = pd.DataFrame(top.most_common(20))\n",
    "negative = negative.iloc[1:,:]\n",
    "negative.columns = ['Common_words','count']\n",
    "negative.style.background_gradient(cmap='Reds')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = px.bar(negative, x=\"count\", y=\"Common_words\", title='Most Commmon Negative(-1) Words', orientation='h', \n",
    "             width=600, height=600)\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation:\n",
    " -"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Most common neutral(0) words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "top = Counter([item for sublist in Neutral['new_lis'] for item in sublist])\n",
    "neutral = pd.DataFrame(top.most_common(20))\n",
    "neutral = neutral.iloc[1:,:]\n",
    "neutral.columns = ['Common_words','count']\n",
    "neutral.style.background_gradient(cmap='Blues')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = px.bar(neutral, x=\"count\", y=\"Common_words\", title='Most Commmon Neutral(0) Words', orientation='h', \n",
    "             width=600, height=600)\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation:\n",
    " -"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Most common news(2) words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "top = Counter([item for sublist in News['new_lis'] for item in sublist])\n",
    "news = pd.DataFrame(top.most_common(20))\n",
    "news = news.iloc[1:,:]\n",
    "news.columns = ['Common_words','count']\n",
    "news.style.background_gradient(cmap='Reds')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = px.bar(news, x=\"count\", y=\"Common_words\", title='Most Commmon Neutral(0) Words', orientation='h', \n",
    "             width=600, height=600)\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation:\n",
    " -"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Counting number of Emojis used in the texts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import emoji\n",
    "emojis_df=[]\n",
    "for string in df['message']:\n",
    "    my_str = str(string)\n",
    "    for each in my_str:\n",
    "        if each in emoji.UNICODE_EMOJI['en'].keys():\n",
    "            emojis_df.append(each)\n",
    "freq = (Counter(i for sub in emojis_df for i in set(sub))) \n",
    "sort_orders = sorted(freq.items(), key=lambda x: x[1], reverse=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(sort_orders)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#function for emoji extraction\n",
    "def ext_emoji(s):\n",
    "    return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'].keys())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#extracting emojis from train data\n",
    "\n",
    "df['emoji'] = df['message'].apply(ext_emoji)\n",
    "df[df['emoji']!='']['emoji']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation:\n",
    "- The most common emoji is the 'face with tears of joy' is an emoji featuring a jovial face laughing, while also crying out tears\n",
    "- We have other emjis that illustrate the impacts of global warmaing on weather and climate change, e.g 'earth,fire,flowers and snowflake'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WordCloud of most common positive words used.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wordcloud = WordCloud(max_font_size=250,background_color='white', width=1500, height=700).generate(' '.join(Positive[Positive['sentiment'] == 1]\n",
    "                                          ['clean_message']))\n",
    "plt.figure( figsize=(16,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('Positive Tweets')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observations:\n",
    " -"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WordCloud of most common negative words used"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wordcloud = WordCloud(max_font_size=250,background_color='white', width=1500, height=700).generate(' '.join(Negative[Negative['sentiment'] == -1]\n",
    "                                          ['clean_message']))\n",
    "plt.figure( figsize=(16,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('Negative Tweets')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observations:\n",
    " -"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WordCloud of most common neutral words used"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wordcloud = WordCloud(max_font_size=250,background_color='white', width=1500, height=700).generate(' '.join(Neutral[Neutral['sentiment'] == 0]\n",
    "                                          ['clean_message']))\n",
    "plt.figure( figsize=(16,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('Neutral Tweets')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observations:\n",
    " -"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WordCloud of most common neutral words used"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wordcloud = WordCloud(max_font_size=250,background_color='white', width=1500, height=700).generate(' '.join(News[News['sentiment'] == 2]\n",
    "                                          ['clean_message']))\n",
    "plt.figure( figsize=(16,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('News Tweets')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WordCloud of most common words used 'Overall'\n",
    "- Wordcloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_msg = \" \".join(tweet for tweet in df.clean_message)\n",
    "train_wordcloud = WordCloud(max_font_size=250,\n",
    "                            background_color=\"black\",\n",
    "                            width=1500,\n",
    "                            height=700,\n",
    "                            collocations=False,\n",
    "                            colormap='Paired').generate(train_msg)\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.imshow(train_wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observation:\n",
    " - "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Balancing Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# Separate minority and majority classes\n",
    "majority_sentiment = df[df['sentiment']== 1]\n",
    "minority_sentiment_0 = df[df['sentiment']== 0]\n",
    "minority_sentiment_2 = df[df['sentiment']== 2]\n",
    "minority_sentiment_neg1 = df[df['sentiment'] == -1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Upsample minority"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "minority_sentiment_0_minority = resample(minority_sentiment_0,\n",
    "                          replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                          n_samples=len(majority_sentiment), # match number in minority class\n",
    "                          random_state=42) # reproducible results\n",
    "\n",
    "minority_sentiment_2_minority = resample(minority_sentiment_2,\n",
    "                                         replace = True,\n",
    "                                         n_samples=len(majority_sentiment),\n",
    "                                         random_state=42)\n",
    "\n",
    "minority_sentiment_neg1_minority = resample(minority_sentiment_neg1,\n",
    "                                   replace=True,\n",
    "                                   n_samples=len(majority_sentiment),\n",
    "                                   random_state=42)\n",
    "\n",
    "# Combine upsampled minority class with majority class\n",
    "df2 = pd.concat([majority_sentiment, minority_sentiment_0_minority,minority_sentiment_2_minority,minority_sentiment_neg1_minority])\n",
    "\n",
    "# Check new class counts\n",
    "df2['sentiment'].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## downsample "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Separate minority and majority classes\n",
    "majority_sentiment = df[df['sentiment']== 1]\n",
    "minority_sentiment_0 = df[df['sentiment']== 0]\n",
    "minority_sentiment_2 = df[df['sentiment']== 2]\n",
    "minority_sentiment_neg1 = df[df['sentiment'] == -1]\n",
    "\n",
    "\n",
    "Pro = resample(majority_sentiment,##PRO,\n",
    "                          replace=False, # sample with replacement (we need to duplicate observations)\n",
    "                          n_samples=len(minority_sentiment_neg1), # match number in minority class\n",
    "                          random_state=42) # reproducible results\n",
    "\n",
    "Neutral = resample(minority_sentiment_0,##NEUTRAL,\n",
    "                                         replace = False,\n",
    "                                         n_samples=len(minority_sentiment_neg1),\n",
    "                                         random_state=42)\n",
    "\n",
    "news = resample(minority_sentiment_2,##NEWS,\n",
    "                                   replace=False,\n",
    "                                   n_samples=len(minority_sentiment_neg1),\n",
    "                                   random_state=42)\n",
    "\n",
    "# Combine upsampled minority class with majority class\n",
    "df_down = pd.concat([Pro,Neutral,news,minority_sentiment_neg1])\n",
    "\n",
    "# Check new class counts\n",
    "df_down['sentiment'].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#n_samples=len(Anti)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show the table of balanced data in clean_text from each language Id\n",
    "dist = df2.groupby('sentiment').count()['clean_message'].reset_index().sort_values(by='clean_message',ascending=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dist"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# UP AND DOWN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Separate minority and majority classes\n",
    "majority_sentiment = df[df['sentiment']== 1]\n",
    "minority_sentiment_0 = df[df['sentiment']== 0]\n",
    "minority_sentiment_2 = df[df['sentiment']== 2]\n",
    "minority_sentiment_neg1 = df[df['sentiment'] == -1]\n",
    "\n",
    "# Downsample majority\n",
    "class_size=round(len(majority_sentiment)/2)\n",
    "Pro_downsampled2 = resample(majority_sentiment,\n",
    "                          replace=False, # sample without replacement (no need to duplicate observations)\n",
    "                          n_samples=class_size, # match number in class size\n",
    "                          random_state=30) # reproducible results\n",
    "\n",
    "# Combine downsampled majority class with minority class\n",
    "#downsampled = pd.concat([not_spam_downsampled, spam])\n",
    "\n",
    "# Upsample minority\n",
    "News_upsampled = resample(minority_sentiment_2,\n",
    "                          replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                          n_samples=class_size, # match number in  class size\n",
    "                          random_state=31) # reproducible results\n",
    "\n",
    "Neutral_upsampled = resample(minority_sentiment_0,\n",
    "                          replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                          n_samples=class_size, # match number in  class size\n",
    "                          random_state=32) # reproducible results\n",
    "\n",
    "Anti_upsampled = resample(minority_sentiment_neg1,\n",
    "                          replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                          n_samples=class_size, # match number in  class size\n",
    "                          random_state=33) # reproducible results\n",
    "\n",
    "\n",
    "\n",
    "# Combine upsampled minority class with majority class\n",
    "upsampled_downsampled = pd.concat([News_upsampled,Neutral_upsampled,Anti_upsampled, Pro_downsampled2])\n",
    "# Check new class counts\n",
    "upsampled_downsampled['sentiment'].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "upsampled_downsampled.head(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprosessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train (Validation) Test Split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['stemm']#Unbalanced"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df2.head()# Upsampled balanced"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = df2['clean_message']\n",
    "y = df2['sentiment']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Upsampling_Downsampling our data makes model performance worse"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "X = upsampled_downsampled['clean_message']\n",
    "y = upsampled_downsampled['sentiment']"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Down sampling our data makes model performance worst\n",
    "\n",
    "we have very little training data here"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "X = df_down['clean_message']\n",
    "y = df_down['sentiment']"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df2.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train_Test Split "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ts = .1\n",
    "rs = 42\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=ts, random_state=rs)\n",
    "\n",
    "experiment.log_parameters({\"test size\": ts, \"random state\": rs})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification Models \n",
    "[Back to Table of Contents](#toc)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "clf_dict = [LinearSVC(random_state=42), KNeighborsClassifier(n_neighbors=5),DecisionTreeClassifier(max_depth=5),\n",
    "                        RandomForestClassifier(max_depth=5, n_estimators=10,\n",
    "                        max_features=1), ComplementNB(), MultinomialNB(), AdaBoostClassifier()\n",
    "                         ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def modeling(clf_dict, X_train, y_train, X_val, y_val):\n",
    "\n",
    "    models = {}\n",
    "    for clf in clf_dict:\n",
    "        clf_text = Pipeline([('tfidf', TfidfVectorizer(min_df=1, max_df=0.1, ngram_range=(1, 2))),('clf', clf)])\n",
    "        \n",
    "        clf_text.fit(X_train, y_train)\n",
    "        predictions = clf_text.predict(X_val)\n",
    "\n",
    "        # Output for each model\n",
    "        models[clf.__class__.__name__] = {\n",
    "            'F1-Macro': metrics.f1_score(y_val,predictions,average='macro'),\n",
    "            'F1-Mccuracy': metrics.f1_score(y_val, predictions,average='micro'),\n",
    "            'F1-Weighted': metrics.f1_score(y_val,predictions,average='weighted')}\n",
    "\n",
    "    return pd.DataFrame.from_dict(models, orient='index')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Displaying all the models\n",
    "classi = modeling(clf_dict, X_train, y_train, X_val, y_val)\n",
    "display_df = classi.sort_values('F1-Macro', ascending=False)\n",
    "display_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance Visuals"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Compare Performance between models visually\n",
    "\n",
    "#fig,axis = plt.subplots(figsize=(10,5))\n",
    "#rmse_x = ['LinearSVC', 'KNeighborsClassifier','DecisionTreeClassifier','RandomForestClassifier', 'ComplementNB', 'MultinomialNB', 'AdaBoostClassifier']\n",
    "#rmse_y = clf_dict\n",
    "#ax = sns.barplot(x=rmse_x, y=rmse_y, palette=('Blues_d'))\n",
    "#plt.title('Weighted F1-Score per Model', fontsize=14)\n",
    "#plt.xticks(rotation=90)\n",
    "#plt.ylabel('Weighted F1-Score')\n",
    "#for p in ax.patches:\n",
    "#    ax.text(p.get_x() + p.get_width()/2, p.get_height(), round(p.get_height(), 2), fontsize=12, ha='center', va='bottom')\n",
    "    \n",
    "#plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running a Single model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "param_grid = {'alpha': [0.1, 1, 5, 11]}  # parameter grid\n",
    "\n",
    "pipeline = Pipeline([('tfidf', TfidfVectorizer(min_df=2, max_df=0.9, ngram_range=(1, 3))), ('mnb', GridSearchCV(MultinomialNB(),  param_grid=param_grid, cv=5, n_jobs=-1, scoring='f1_weighted'))])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipeline.fit(X_train,y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions = pipeline.predict(X_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(classification_report(predictions,y_val))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f1_Macro = metrics.f1_score(y_val,predictions,average='macro')\n",
    "f1_Mccuracy = metrics.f1_score(y_val, predictions,average='micro'),\n",
    "f1_Weighted =  metrics.f1_score(y_val,predictions,average='weighted')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "experiment.log_metric(\"f1-Macro\", f1_Macro)\n",
    "experiment.log_metric(\"f1-Mccuracy\", f1_Mccuracy)\n",
    "experiment.log_metric(\"f1-Weighted\", f1_Weighted)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction Submission csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "LSVC = pd.DataFrame(data={'tweetid': test_df['tweetid'],'sentiment': predictions})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "LSVC.to_csv(\"GridSearch_MultNB_Clean_message.csv\", index=False)\n",
    "#OUR MODEL IMPROVED BY USING STEMMA BETTER THAN LEMMA AND WE USED UPSAMPLED DATA WITH TRAINING DATA OF 90%"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "experiment.end()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "- Websites used for insipiration"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.037324,
     "end_time": "2021-06-10T11:02:46.044445",
     "exception": false,
     "start_time": "2021-06-10T11:02:46.007121",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- analyticsvidhya: https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/\n",
    "- towardsdatascience: https://towardsdatascience.com/how-to-efficiently-remove-punctuations-from-a-string-899ad4a059fb\n",
    "- codegrepper:  https://www.codegrepper.com/code-examples/python/pandas+series+remove+punctuation\n",
    "- https://stackoverflow.com/questions/54396405/how-can-i-preprocess-nlp-text-lowercase-remove-special-characters-remove-numb\n"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.037772,
     "end_time": "2021-06-10T11:02:46.119164",
     "exception": false,
     "start_time": "2021-06-10T11:02:46.081392",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "papermill": {
     "duration": 0.0364,
     "end_time": "2021-06-10T11:02:46.193094",
     "exception": false,
     "start_time": "2021-06-10T11:02:46.156694",
     "status": "completed"
    },
    "tags": []
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2cb31fa55ae00465923d9b187a5051f66c494d866be42a4c03d35b8bacfe6992"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('.env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 145.811705,
   "end_time": "2021-06-10T11:02:47.040917",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-10T11:00:21.229212",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}